---
title: 多项式回归
author: Heng-Heng Shen
github: rh01
date: '2017-12-21'
slug: polynomial-regression
categories:
  - ml
tags:
  - matrix
  - ml
---



<p>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第二周:<strong>多项式回归</strong>的课堂记录，有了合适的特征之后，我们发现线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，因此引出多项式回归模型，主要讲解了以下几个内容:</p>
<ul>
<li>将多项式回归模型转换为线性模型</li>
<li>正规方程</li>
</ul>
<section class="level1">
<h1>将多项式回归模型转换为线性模型</h1>
<p>什么是多项式模型？比如一个二次模型<span class="math display">\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2\]</span>,三次模型<span class="math display">\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2 + \theta_{3}x^3\]</span></p>
<p>通常情况下，在模型进行选择时，首先要观察数据，然后再决定准备尝试什么样的模型，比如下面图 中的数据分布，我们可以通过观察可以看到，数据的分布用线性模型并不能达到很好的效果，所以在这里尝试用一个多项式模型来拟合我们的训练数据。</p>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/67294778.jpg" /></p>
<p>可以尝试多项式函数表示我们的模型：</p>
<p><span class="math display">\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2\]</span></p>
<p>或者</p>
<p><span class="math display">\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2 + \theta_{3}x^3\]</span></p>
<p>或者</p>
<p><span class="math display">\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^{\frac{1}{2}}\]</span></p>
<p>有趣的是，如果我们对这些高次项进行替换，比如在<span class="math inline">\(h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2\)</span>中，将 <span class="math display">\[x_{2} := x^{2}\]</span> 将可以转换为<span class="math inline">\(h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x_{2}\)</span>，这样转换为多变量线性回归模型了。</p>
<p><strong>注意：</strong>在采用多项式回归模型，运行梯度下降算法前，特征缩放非常有必要！</p>
</section>
<section class="level1">
<h1>正规方程</h1>
<p>目前为止，都在利用梯度下降算法进行优化我们的代价函数，但对于某些的现性回归问题，正规方程是更好的解决方案。不像梯度算法那样，求解出参数<span class="math inline">\(\theta\)</span>需要进行很多次迭代才能求出，而正规方程直接<strong>一次性</strong>求出参数！其实正规方程的背后就是借助于来做的，即通过求解下面的方程找出使得代价函数最小的参数.</p>
<p><span class="math display">\[\frac{\partial}{\partial_{j}}J(\theta_{j}) = 0 \]</span></p>
<p>比如：<span class="math inline">\(J(\theta) = a\theta^{2} + b\theta + c\)</span>，那么通过<span class="math inline">\(\frac{d}{d_{\theta}}J(\theta) = ... = 0\)</span>可以求解出<span class="math inline">\(\theta\)</span>来.</p>
<p><strong>经验：</strong> 使用正规方程<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>解出向量<span class="math display">\[\theta = (x^{T}x)^{-1}x^{T}y\]</span></p>
<section class="level2">
<h2>利用正规方程求解的例子</h2>
<p>给你一组训练数据，<span class="math inline">\(m\)</span> = 4, <span class="math inline">\(n\)</span> = 4，则<span class="math inline">\(X \in \mathbb{R}^{4*5}\)</span>, <span class="math inline">\(y \in \mathbb{R}^{4}\)</span>，数据如图</p>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/66125159.jpg" /></p>
<p>用矩阵可以表示为： <span class="math display">\[X = \left[\begin{matrix}
1&amp;2104&amp;5 &amp;1&amp;45\\
1&amp;1416&amp; 3&amp; 2&amp; 40\\
1&amp;1534&amp; 3 &amp;2&amp; 30 \\
1&amp;852&amp; 2&amp; 1&amp; 36
\end{matrix}
\right],
y = \left[\begin{matrix}
460\\
232\\
315\\
178
\end{matrix}\right]\]</span></p>
<p>那么就可以使用经验公式<span class="math inline">\(\theta = (x^{T}x)^{-1}x^{T}y\)</span>求解了！</p>
<p><strong>警告：</strong> 往往在使用公式求解时，存在不可逆矩阵，导致求逆矩阵失败，主要<strong>原因</strong>为，特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能特征数量大于训练集大于训练集数量，正规方程方法不能用！</p>
</section>
<section class="level2">
<h2>梯度下降算法与正规方程的比较</h2>
<p>梯度下降算法：</p>
<ul>
<li>不需要选择<span class="math inline">\(\alpha\)</span></li>
<li>需要很多次迭代</li>
<li>当特征数量<span class="math inline">\(n\)</span>比较大时，也能很好的适应</li>
<li>适应各种类型的数据</li>
</ul>
<p>正规方程：</p>
<ul>
<li>不需要选择<span class="math inline">\(\alpha\)</span></li>
<li>一次计算即可得出结果，不需要很多次迭代</li>
<li>需要计算<span class="math inline">\((x^{T}x)^{-1}\)</span>运算代价大，时间效率为<span class="math inline">\(O(n^3)\)</span>，但是如果n&lt;10000时可以接受！</li>
<li>只适应于现行模型，不适用与逻辑回归等其他类型</li>
<li>不需要归一化特征变量</li>
</ul>
</section>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>最小二乘解<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>
