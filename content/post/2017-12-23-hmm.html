---
title: 隐马尔可夫模型
author: Heng-Heng Shen
github: rh01
date: '2017-12-23'
slug: hmm
categories:
  - blog
  - ml
  - hmm
tags:
  - hmm
  - ml
  - blog
---



<p>很早就想写一篇关于<strong>隐马尔可夫模型</strong>的文章了，这次刻意的将模型及其有关算法复习了一下，才有了这个信心去写了这篇文章。这篇文章主要参考了李航老师的《统计机器学习》 部分内容和徐亦达老师在油管上的教程<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>。并且这里的隐马尔可夫模型主要是指<strong>离散形式的动态模型</strong>。</p>
<p>隐马尔可夫模型主要是一种<strong>可用于标注问题的统计学习模型</strong>，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型。近些年来主要用于语音信号处理，自然语言处理，生物信息，金融分析等领域，该教程涉及很多概率计算问题，所以希望读者能够有概率背景的情况下，阅读更佳。</p>
<p>本部分主要介绍一下几个部分：</p>
<ul>
<li>HMM现象</li>
<li>马尔可夫过程/链</li>
<li>算法
<ul>
<li>直接计算法</li>
<li>前向-后向算法
<ul>
<li>前向算法</li>
<li>后向算法</li>
</ul></li>
</ul></li>
<li>学习算法</li>
<li>总结</li>
</ul>
<section id="hmm" class="level1">
<h1>HMM现象</h1>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/13276902.jpg" /></p>
<p>玩股票的朋友都知道，图右，在市场规则下，股民只知道股票的涨停和不变这三种表面现象，而人们并不希望仅仅知道市场的涨停，而是想知道市场隐藏的信息，比如现在股票市场是熊市还是牛市，因为知道这些隐藏的信息之后，我们可以利用这些信息去长期跟进还是及时退出，以防市场大变。在这里我们称这种人们表面上能观察出来的现象为观测（observation），而那些隐藏的变量被称作状态。</p>
<p>有了上面比较直观的介绍以后，我们就可以定义什么是隐马尔可夫模型了。</p>
<p><strong>定义10.1（隐马尔可夫模型）</strong>　隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列（state sequence）；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence）。序列的每一个位置又可以看作是一个时刻。</p>
<p>比如：上图的Bull-&gt;Bear-&gt;Event就是其中一个状态序列，而其产生的观测形成的序列up-&gt;down-&gt;unchange被称作观测序列，而这些序列在一定假设下，具有非常好的概率性质。</p>
</section>
<section class="level1">
<h1>马尔可夫链</h1>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/10240319.jpg" /></p>
<p>上图是隐马尔可夫的概率图模型，概率图反应了事件的依赖和独立关系。</p>
<p>首先我们要说明q表示状态，不可观测。y表示可观测的现象。假设状态与状态之间、状态与观测现象之间均满足马尔可夫过程，其中该过程要求具备“无记忆”的性质（马尔可夫性质）：下一状态的概率分布只能由当前状态决定，在时间序列中前面的事件均与之无关。 用数学语言表示为：</p>
<p><span class="math display">\[\Pr(X_{{n+1}}=x\mid X_{1}=x_{1},X_{2}=x_{2},\ldots ,X_{n}=x_{n})=\Pr(X_{{n+1}}=x\mid X_{n}=x_{n})\]</span></p>
<p>那么根据概率图模型，我们得出了两个重要的公式：</p>
<p><strong>离散</strong>状态转移概率： <span class="math display">\[p(q_t |q_1, . . . , q_{t−1}, y_1, . . . , y_{t−1}) = p(q_t |q_{t−1})\]</span></p>
<p><strong>离散/连续</strong>观测概率： <span class="math display">\[p(y_t |q_1, . . . , q_{t−1}, y_1, . . . , y_{t−1})  = p(y_t |q_t )\]</span></p>
那么有了这个公式我们可以计算出图的转移概率矩阵，计算过程见图

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/98038385.jpg" /></p>
<p><span class="math inline">\(A\)</span>是状态转移概率矩阵,<span class="math inline">\(A\)</span>描述了状态之间转换关系及其分布。</p>
<p>其中<span class="math inline">\(a_{ij}=P(i_{t+1}=q_j|i_{t}=q_i),\; \; i=1,2,...,N;j=1,2,...,N;\)</span>， 是在时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_i\)</span>的条件下在时刻<span class="math inline">\(t+1\)</span>转移到状态<span class="math inline">\(q_j\)</span> 的概率。</p>
同样地，我们可以计算出观测概率矩阵，具体计算过程见

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/34045629.jpg" /></p>
<p><span class="math inline">\(B\)</span>是观测概率矩阵, 其中<span class="math inline">\(b_j(k)=P(o_{t}=v_k|i_{t}=q_j)\; \; k=1,2,...,M;j=1,2,...,N;\)</span>， 是在时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_j\)</span>的条件下生成观测<span class="math inline">\(v_k\)</span>的概率。</p>
<p>有了这些性质之后我们就可以计算观测序列的概率了。比如利用图和图计算<span class="math inline">\(Pr(y_1=up, y_2 = up, y_3=down)\)</span>的概率.</p>
<span class="math display">\[\begin{align*}
P(y_1,y_2,y_3) &amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_1,y_2,y_3,q_1,q_2,q_3) \\
&amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_3|y_1,y_2,q_1,q_2,q_3)P(y_1,y_2,q_1,q_2,q_3) \\
&amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_3|q_3)P(y_1,y_2,q_1,q_2,q_3) \\
&amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_3|q_3)P(q_3|y_1,y_2,q_1,q_2)P(y_1,y_2,q_1,q_2) \\
&amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_1,y_2,q_1,q_2) \\
&amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_2|y_1,q_1,q_2)P(y_1,q_1,q_2) \\
&amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_2|q_2)P(y_1,q_1,q_2) \\
&amp;= \sum_{q_1=1}^{k} \sum_{q_2=1}^{k} \sum_{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_2|q_2)P(q_2|q_1)P(y_1|q_1)P(q_1) 
\end{align*}\]</span>
<p>但是发现除了A和B我们已知，还无法计算<span class="math inline">\(P(y_1,y_2,y_3)\)</span>，还需要知道<span class="math inline">\(P(q_1)\)</span>的分布情况，所以在这里引出了另外一个条件，<span class="math inline">\(P(q_1)\)</span>的分布。也就是初始状态概率。为了符号化，这里<span class="math inline">\(\pi\)</span>是初始状态概率向量<span class="math inline">\(\pi = (\pi_i)\)</span>： 其中<span class="math inline">\(\pi_i = P(i_1 = q_i)\)</span>， 是时刻t＝1处于状态<span class="math inline">\(q_i\)</span> 的概率。</p>
<p><strong>总结：</strong> 隐马尔可夫模型由初始状态概率向量<span class="math inline">\(\pi\)</span>、状态转移概率矩阵<span class="math inline">\(A\)</span>和观测概率矩阵<span class="math inline">\(B\)</span>决定。<span class="math inline">\(\pi\)</span>和<span class="math inline">\(A\)</span>决定状态序列，<span class="math inline">\(B\)</span>决定观测序列。因此，隐马尔可夫模型<span class="math inline">\(\lambda\)</span>可以用三元符号表示，即 <span class="math display">\[\lambda=(A,B,\pi)\]</span> <span class="math inline">\(A\)</span>,<span class="math inline">\(B\)</span>,<span class="math inline">\(\pi\)</span>称为隐马尔可夫模型的三要素。</p>
</section>
<section class="level1">
<h1>算法</h1>
<p>HMM的三个基本计算问题： <span class="math display">\[\begin{align*}
 &amp;\text{Evaluate} \; \; p(Y|\lambda) \\
 &amp;\lambda_{MLE} = \text{argmax}_{\lambda}p(Y|\lambda) \\
 &amp;\text{argmax}_Q p(Y|Q,\lambda) 
\end{align*}\]</span></p>
<section class="level2">
<h2>直接计算法</h2>
<p>首先解决第一个问题，<span class="math inline">\(\text{Evaluate} \; \; p(Y|\lambda)\)</span>,</p>
<span class="math display">\[\begin{align*}
P(Y|\lambda) &amp;= \sum_{Q}[p(Y,Q|\lambda)] =\sum_{q_1=1}^{k} ..., \sum_{q_T=1}^{k} [p(y_1,...,y_T,q_1,...,q_T|\lambda)] \\
&amp;=\sum_{q_1=1}^{k} ..., \sum_{q_T=1}^{k} [p(y_1,...,y_T,q_1,...,q_T|\lambda)] \\
&amp;= \sum_{q_1=1}^{k} ...,\sum_{q_3=1}^{k} p(q_1)p(y_1|q_1)p(q_2|q_1)...p(q_t|q_{t-1}p(y_t|q_t))\\
&amp;= \sum_{q_1=1}^{k} ...,\sum_{q_3=1}^{k} \pi(p_1)\amalg_{t=2}^{T}a_{q_{t-1},q_t}b_{q_t}(y_t)
\end{align*}\]</span>
<p>其中: <span class="math display">\[\begin{align*}
p(q_t = j|q_{t−1} = i) &amp;\equiv a_{i,j} \\
p(y_t |q_t = j) &amp;\equiv bj (y_t)
\end{align*}\]</span> 但是，利用上面公式计算量很大，是<span class="math inline">\(O(TN^T)\)</span>阶的，这种算法不可行。</p>
</section>
<section id="-" class="level2">
<h2>前向-后向算法</h2>
<section class="level3">
<h3>前向算法</h3>
<p><strong>定义10.2（前向概率）</strong>　给定隐马尔可夫模型，定义到时刻t部分观测序列为<span class="math inline">\(o_1,o_2,…,o_t\)</span>且状态为<span class="math inline">\(q_i\)</span>的概率为前向概率，记作 <span class="math display">\[\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_t|\lambda)\]</span></p>
<p>下面是前向算法的概率图模型。</p>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/35121312.jpg" /></p>
<p>前向过程： <span class="math display">\[\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_t|\lambda) \Longrightarrow p(Y|\lambda) = \sum_{i=1}^{k} \alpha_i(T)\]</span></p>
<p>递推:对<span class="math inline">\(t＝1,2,…，T-1\)</span>，</p>
<span class="math display">\[\begin{align*}
\alpha_i(1) &amp;= p(y_1,q_1=i|\lambda) = p(q_1)p(y_1|q_1)=\pi_i b_i(y_1) \\
\alpha_j(2) &amp;= p(y_1,y_2,q_2=j|\lambda) = \sum_{i=1}^{k}p(q_1=i)p(y1|q_1=i)p(q_2 = i | q_1 = i)p(y_2 | q_2 = j) \\
&amp;= [\sum_{i=1}^k \alpha_i(1)\alpha_{i,j}]b_j(y_2) = p(q_1)p(y_1|q_1)=\pi_ib_i(y_1) \\ 
&amp;... \\
\alpha_j(t+1) &amp;=[\sum_{i=1}^k \alpha_i(t)\alpha_{i,j}]b_j(y_{t+1}) \\
&amp;... \\
\alpha_j(T) &amp;=[\sum_{i=1}^k \alpha_i(T-1)\alpha_{i,j}]b_j(y_{T})
\end{align*}\]</span>
</section>
<section class="level3">
<h3>后向算法</h3>
<p><strong>定义10.3（后向概率）</strong>　给定隐马尔可夫模型，定义在时刻t状态为<span class="math inline">\(q_i\)</span>的条件下，从t+1到T的部分观测序列为<span class="math inline">\(o_{t+1}\)</span>,<span class="math inline">\(o_{t+2}\)</span>,…,<span class="math inline">\(o_T\)</span>的概率为后向概率，记作 <span class="math display">\[\beta_t(i)=P(o_{t+1},o_{t+2},o_{t+3},...,o_T|i_t=q_i,\lambda)\]</span></p>
<p>可以用递推的方法求得后向概率<span class="math inline">\(\beta_t(i)\)</span>及观测序列概率<span class="math inline">\(P(O|\lambda)\)</span>。 <span class="math display">\[\beta_i (t) = p(y_{t+1}, . . . , y_T |q_t = i, \lambda)\]</span> 下面是后向算法的概率图模型。</p>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/98688168.jpg" /></p>
<p>后向过程： <span class="math display">\[\beta_i (t) = p(y_{t+1}, . . . , y_T |q_t = i, \lambda) \Longrightarrow \sum_{i=1}^k \beta_i(1)\pi_ib_i(y_1)=p(Y|\lambda)\]</span></p>
<p>迭代过程：</p>
<span class="math display">\[\begin{align*}
\beta_i(T) &amp;= 1 \\
\beta_i(T-1) &amp;= p(y_T|q_{T-1} = i) = \sum_{j=1}^kp(q_T=j|q_{T-1}=i)p(y_T|q_T=j)=\sum_{j=1}^{k}a_{i,j}b_j(T) \\
\beta_i(T-2) &amp;= p(y_T,y_{T-1}|q_{T-2} = i) \\
&amp;= \sum_{j=1}^k \sum_{l=1}^k p(q_T=l|q_{T-1}=j)p(y_T|q_T=l) p(q_{T-1}=j|q_{T-2}=i)p(y_{T-1}|q_{T-1}=j)\\
&amp;=\sum_{j=1}^{k}a_{i,j}b_j(y_{T-1})\beta_j(T-1) \\
&amp;...\\
\beta_i(t) &amp;=\sum_{j=1}^k\alpha_{i,j}b_j(y_t+1)\beta_j(t + 1) \\
&amp;...\\
\beta_i(1)&amp;=\sum_{j=1}^k\alpha_{i,j}b_j(y_2)\beta_j(2)
\end{align*}\]</span>
<p>在时刻t处位于序列<span class="math inline">\(Y\)</span>状态<span class="math inline">\(q_i\)</span>时的概率：</p>
<p><span class="math display">\[p(q_t = i|Y,\lambda) = \frac{p(Y, q_t = i|\lambda)}{p(Y|\lambda)}=\frac{p(Y, q_t = i|\lambda)}{\sum_{j=1}^{k}p(Y, q_t =j|\lambda)}= \frac{\alpha_i(t)\beta_i{t}}{\sum_{j=1}^{k}\alpha_j(t)\beta_j(t)}\]</span></p>
<span class="math display">\[\begin{align*}
p(Y, q_t = i|\lambda) &amp;= p(Y|q_t = i)p(q_t = i)\\
&amp;= p(y_1, . . . y_t |q_t = i)p(y_{t+1},... y_T |q_t = i)p(q_t = i) \\
&amp;= p(y_1, . . . y_t , q_t = i)p(y_{t+1},... y_T |q_t = i) \\
&amp;= \alpha_i(t)\beta_i(t)
\end{align*}\]</span>
</section>
</section>
</section>
<section class="level1">
<h1>学习算法</h1>
<p>隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与非监督学习实现。主要讲解非监督学习算法——Baum-Welch算法（也就是EM算法）。</p>
<p>复习下EM算法： <span class="math display">\[{\theta ^{(g + 1)}} = \mathop {\arg \max }\limits_\theta  [Q(\theta ,{\theta ^{(g)}})] = \mathop {\arg \max }\limits_\theta  (\int_Z {\log (p(X,Z|\theta ))P(Z|X,{\theta ^{(g)}})dz} )\]</span></p>
<p>在HMM中，我们把要求解的问题写成E-M的形式，如下所示： <span class="math display">\[{\lambda ^{(g + 1)}} = \mathop {\arg \max }\limits_\lambda  \left( {\underbrace {\int_{q \in Q} {\ln (p(Y,q|\lambda ))P(q,Y|{\lambda ^{(g)}})dq} }_{Q(\lambda ,{\lambda ^{(g)}})}} \right)\]</span></p>
<p>其中， <span class="math display">\[\begin{align*}
Q(\lambda ,{\lambda ^{(g)}}) &amp;= \int_{q \in Q} {\ln (p(Y,q|\lambda ))P(q,Y|{\lambda ^{(g)}})dq}  \\ 
&amp;= \sum\limits_{{q_0} = 1}^k  \cdots  \sum\limits_{{q_T} = 1}^k {\left( {\ln {\pi _0} + \sum\limits_{t = 1}^T {\ln {a_q}_{_{t - 1},{q_t}} + \sum\limits_{t = 1}^T {\ln {b_{{q_t}}}({y_t})} } } \right)} p(q,Y|{\lambda ^{(g)}}) 
\end{align*}\]</span></p>
<p>可以看到公式里面有三个元素，现在依次求解每一项。</p>
<p>首先求解第一项<span class="math inline">\(\sum\limits_{{q_0} = 1}^k \cdots \sum\limits_{{q_T} = 1}^k {\ln {\pi _0}} p(q,Y|{\lambda ^{(g)}})\)</span>， <span class="math display">\[\sum\limits_{{q_0} = 1}^k  \cdots  \sum\limits_{{q_T} = 1}^k {\ln {\pi _0}} p(q,Y|{\lambda ^{(g)}}){\rm{ = }}\sum\limits_{i = 1}^k {\ln {\pi _i}p({q_0} = i,Y|{\lambda ^{(g)}})}\]</span> 其中约束条件为<span class="math inline">\(\sum\limits_{i = 1}^k {{\pi _i}} = 1\)</span>。</p>
<p>那么这种带有约束的最优化问题常用拉格朗日乘数法进行求解，即： <span class="math display">\[{\mathbb{LM}^{{\rm{term1}}}}{\rm{ = }}\sum\limits_{i = 1}^k {\ln {\pi _i}p({q_0} = i,Y|{\lambda ^{(g)}})} {\rm{ + }}\tau \left( {\sum\limits_{i = 1}^k {{\pi _i}}  - 1} \right)\]</span></p>
<p><span class="math display">\[{{\partial {\mathbb{LM}^{{\rm{term1}}}}} \over {\partial {\pi _i}}} = {{p(q,Y|{\lambda ^{(g)}})} \over {{\pi _i}}} + \tau  = 0  \; \; \; \; \; \; \; \;  {{\partial {^{{\rm{term1}}}}} \over {\partial \tau }} = \sum\limits_{i = 1}^k {{\pi _i}}  - 1 = 0 
\]</span> <span class="math display">\[p({q_0} = i,Y|{\lambda ^{(g)}}) =  - \tau {\pi _i} \Longrightarrow \sum\limits_{i = 1}^k {p({q_0} = i,Y|{\lambda ^{(g)}})}  =  - \tau \sum\limits_{i = 1}^k {{\pi _i}}  =  - \tau\]</span></p>
<p>因此有 <span class="math display">\[{\pi _i} = {{p({q_0} = i,Y|{\lambda ^{(g)}})} \over { - \tau }} \Longrightarrow {\pi _i} = {{p({q_0} = i,Y|{\lambda ^{(g)}})} \over {\sum\limits_{i = 1}^k {p({q_0} = i,Y|{\lambda ^{(g)}})} }}\]</span></p>
<p>下面求解第二项<span class="math inline">\(\sum\limits_{{q_0} = 1}^k \cdots \sum\limits_{{q_T} = 1}^{k{\rm{ }}} {\sum\limits_{t = 1}^T {\ln {a_{{q_{t - 1,}}{q_t}}}} } p(q,Y|{\lambda ^{(g)}}) = \sum\limits_{i = 1}^k {\sum\limits_{j = 1}^k {\sum\limits_{t = 1}^T {\ln {a_{i,j}}p({q_{t - 1}} = i,{q_t} = j,Y|{\lambda ^{(g)}})} } }\)</span></p>
<p>同样的是拉格朗日优化问题，此时的拉格朗日函数为 <span class="math display">\[\mathbb{LM}^{\text{term2}} = \sum\limits_{i = 1}^k {\sum\limits_{j = 1}^k {\sum\limits_{t = 1}^T {\ln {a_{i,j}}p({q_{t - 1}} = i,{q_t} = j,Y|{\lambda ^{(g)}})} } }  + \sum\limits_{i = 1}^k {{\tau _i}\left( {\sum\limits_{i = 1}^k {{\pi _i}}  - 1} \right)}\]</span></p>
<p>第三项<span class="math inline">\(\sum\limits_{{q_0} = 1}^k \cdots \sum\limits_{{q_T} = 1}^k {\sum\limits_{t = 1}^T {\ln {b_{{q_t}}}({y_t})} } p(q,Y|{\lambda ^{(g)}}) = \sum\limits_{i = 1}^k {\sum\limits_{j = 1}^k {\sum\limits_{t = 1}^T {\ln {b_j}({y_t})p({q_t},Y|{\lambda ^{(g)}})}}}\)</span>的拉格朗日优化目标函数为</p>
<p><span class="math display">\[\mathbb{LM}^{\rm{term3}} = \sum\limits_{i = 1}^k {\sum\limits_{j = 1}^k {\sum\limits_{t = 1}^T {\ln {b_j}({y_t})p({q_t},Y|{\lambda ^{(g)}})} } }  + \sum\limits_{i = 1}^k {{\tau _i}\left( {\sum\limits_{i = 1}^k {{b_j}({y_t})}  - 1} \right)}\]</span></p>
<p>具体解法和第一项的求解类似！</p>
</section>
<section class="level1">
<h1>总结</h1>
<p>隐马尔可夫模型作为动态模型的一种，主要对时间序列的数据进行建模，在学习算法上主要是利用E-M算法进行求解，但是总体上大大降低了计算效率，目前只介绍了离散型的动态模型，后期如果有时间，会具体介绍有关连续性的动态模型算法。</p>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLyAft-JyjIYoc9LN241WKqLPuggfSBBpt" class="uri">https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLyAft-JyjIYoc9LN241WKqLPuggfSBBpt</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>
