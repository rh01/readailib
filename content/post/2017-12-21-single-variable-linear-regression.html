---
title: 单变量线性回归
author: Heng-Heng Shen
github: rh01
date: '2017-12-21'
slug: single-variable-linear-regression
categories:
  - blog
  - comment
  - ml
tags:
  - ml
---



<p>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第二周:<strong>单变量线性回归</strong>的课堂记录，主要讲解了以下几个内容:</p>
<ul>
<li>代价函数</li>
<li>梯度下降算法</li>
</ul>
<section class="level1">
<h1>模型表达</h1>
还是上一个关于房屋价格预测的例子，给你一组房屋的大小和对应房屋的价格的数据，让你对数据进行建模，使得对于其他的房屋的大小更好的拟合出更准确的价格出来。如图1所示。

<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/50192377.jpg" />
<p class="caption">
图 1: 房屋价格预测模型
</p>
<p>题外话：首先数据是一组关于标准输入与标准答案的数据集，那么对该数据进行建模，是属于监督学习任务。又因为数据只含有一个变量（<span class="math inline">\(size\)</span>）即一个特征，且找到一条直线来拟合数据集，所以该问题又被称为单变量线性回归问题。</p>
图2是部分的样本数据，其中<span class="math inline">\(X\)</span>表示房屋的大小，<span class="math inline">\(Y\)</span>表示对应房屋的价格。

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/25995934.jpg" />
</center>
<p class="caption">
图 2: 数据
</p>
<p>为了后面更好的描述，所以对一些符号进行声明：</p>
<ul>
<li><span class="math inline">\(m\)</span> 训练集中实例的数量</li>
<li><span class="math inline">\(x&#39;s\)</span> 输入变量或者特征</li>
<li><span class="math inline">\(y&#39;s\)</span> 输出变量或目标变量</li>
<li><span class="math inline">\((x,y)\)</span> 一个训练样本</li>
<li><span class="math inline">\((x^{(i)},y^{(i)})\)</span> 第<span class="math inline">\(i\)</span>个训练样本 \end{itemize}</li>
</ul>
有了数据之后，可以用下图就可以描述房屋价格预测问题建模的过程。

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/49786727.jpg" />
</center>
<p class="caption">
图 3: 建模的过程
</p>
<p>图片描述：因而解决房屋价格预测问题，实际上就是要将训练集”喂给“我们的学习算法，进而学习到一个假设<span class="math inline">\(h\)</span>，然后将要预测的房屋尺寸作为输入变量输入给<span class="math inline">\(h\)</span>，预测出该房屋的交易价格作为输出结果。<strong>其实<span class="math inline">\(h\)</span>就是一个我们要学习的函数</strong>。</p>
<p>我们如何表示函数呢？对于单变量线性回归问题来说，<span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1} x\)</span>.</p>
</section>
<section class="level1">
<h1>代价函数</h1>
<section class="level2">
<h2>引入代价函数</h2>
<p>有了训练数据，也有了模型的表示<span class="math inline">\(h\)</span>，但是问题是如何选择 <span class="math inline">\(\theta_{0}\)</span> 和 <span class="math inline">\(\theta_{1}\)</span> ?</p>
<p>基本想法是选择<span class="math inline">\(\theta_{0}\)</span>，<span class="math inline">\(\theta_{1}\)</span>以使得在训练集上<span class="math inline">\(h_{\theta}(x)\)</span>接近<span class="math inline">\(y\)</span>.用数学表达式表示出来，如下： <span class="math display">\[minimize_{\theta{0},\theta_{1}}\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{(i)})^{2}\]</span></p>
<p><strong>注意：</strong> 在学习的时候，有一个区别，<span class="math inline">\(loss function\)</span>描述的是单个样本误差，而<span class="math inline">\(cost\)</span> <span class="math inline">\(function\)</span>描述整个训练集的误差。</p>
<p>同样地，为了更好的下文描述，将其简化：</p>
<p><span class="math display">\[J(\theta_{0},\theta_{1}) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{(i)})^{2}\]</span></p>
<p>Goals: <code>$$ minimize_{\theta{0},\theta{1}}J(\theta_{0}, \theta_{1})$$</code> 其中<span class="math inline">\(J(\theta_{0}, \theta_{1})\)</span>是平方误差（代价）函数，它是解决回归问题常用的手段。</p>
</section>
<section id="j" class="level2">
<h2>代价函数<span class="math inline">\(J\)</span>的工作原理</h2>
<ul>
<li>假说函数: <span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x\)</span></li>
<li>参数：<span class="math inline">\(\theta_{0}, \theta_{1}\)</span></li>
<li>代价函数：<span class="math inline">\(J(\theta_{0},\theta_{1}) = frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{(i)})^{2}\)</span></li>
<li>目标：<span class="math inline">\(minimize_{\theta{0},\theta{1}}J(\theta_{0}, \theta_{1})\)</span></li>
</ul>
<p>简化来讲，就是找到使得误差最小的那一对参数（<span class="math inline">\(\theta_{0}, \theta_{1}\)</span>).</p>

<p class="caption">
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/26673069.jpg" /> 图 4: 代价函数工作原理
</p>
</section>
<section class="level2">
<h2>代价函数的直观理解</h2>
<p>引入一种表示方法：轮廓图。我们在三维空间来表示（<span class="math inline">\(\theta_{0}, \theta_{1}, J(\theta_{0}, \theta_{1})\)</span>)，如图</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/12948998.jpg" />
</center>
<p class="caption">
图 5: 轮廓图
</p>
<p>可以直观地得到以下结论：在曲面的最低的那个点对应的代价误差最小，对应的（<span class="math inline">\(\theta_{0}, \theta_{1}\)</span>)是我们要找的参数组合。</p>
<p>还有一种表示方法：等高线图，他的工作原理是，每一圈上的值都是相等的，即对应的代价误差是相同的。从外到里，代价不断减小，最里面的是最小的代价误差，也是我们算法要最终学习到的点。如图</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/66180792.jpg" />
</center>
<p class="caption">
图 6: 等高线图
</p>
</section>
</section>
<section class="level1">
<h1>梯度下降算法</h1>
<section class="level2">
<h2>算法描述</h2>
<p>我们很迫切想求得最佳拟合参数（<span class="math inline">\(\theta_{0}, \theta_{1}\)</span>），但是有不想通过枚举的方式求得，因此需要一个算法能够自动求出使得代价函数<span class="math inline">\(J(\theta_{0}, \theta_{1})\)</span>取得最小值的参数<span class="math inline">\(\theta_{0}, \theta_{1}\)</span>.</p>
<p>所以在这里引入梯度下降算法。主要思想是想象你在一个山丘上，怎么样以最快的速度从山上到山脚下？在这里就引入了梯度的概念，即下降速度最快的方向，所以人没走一步就检查一下，当前是否为下降最快的方向？若不是则将重新求梯度，按照梯度指示的方向走，则为最快的方式！将这种思想应用到求使得代价误差函数最小，也是这么做的。但是这样往往带来几个问题？</p>
<ol type="1">
<li>以多大的脚步往下走，因为如果脚步过大，则会造成错过最佳的下山路线。（学习速率选择问题）</li>
<li>由于不一定你的目标函数是凸函数，所以每次走可能走到不同”的山脚“。（局部最小值，凸优化问题）如图</li>
</ol>
<p>这些问题将会在后面一一解决！</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/65682294.jpg" />
</center>
<p>下面是梯度下降算法的为伪代码。</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/96169106.jpg" />
</center>
<p>下图 直观的描述<span class="math inline">\(\theta\)</span>是如何变化的.这里<span class="math inline">\(\alpha &gt; 0\)</span>.</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/29605590.jpg" />
</center>
</section>
<section id="alpha" class="level2">
<h2>学习率<span class="math inline">\(\alpha\)</span></h2>
学习率在算法迭代时起着很大的作用。


<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/75838488.jpg" />
</center>
<p>虽然选择的是合适的<span class="math inline">\(alpha\)</span>，但是算法还是会掉到局部最小值，对于局部最小值的描述如图</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/23570380.jpg" />
</center>
<p>在算法迭代时，随着逐渐接近最小值，步子会慢慢地变小，直到接近最小值。所以在算法的迭代过程中不需要对<span class="math inline">\(\alpha\)</span>改变。</p>

<ul>
<li>假说函数: <span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x\)</span></li>
<li>参数：<span class="math inline">\(\theta_{0}, \theta_{1}\)</span></li>
<li>代价函数：<span class="math inline">\(J(\theta_{0},\theta_{1}) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{(i)})^{2}\)</span></li>
<li>目标：<span class="math inline">\(minimize_{\theta{0},\theta{1}}J(\theta_{0}, \theta_{1})\)</span></li>
<li>求解：<span class="math inline">\(\theta_{j} := \theta_{j} - \alpha \frac{\partial}{\partial_{\theta_{j}}} J(\theta_{0}, \theta_{1})\)</span> for <span class="math inline">\(j=0\)</span> 和 <span class="math inline">\(j=1\)</span></li>
</ul>
<p>结合上面的公式可以求得 <span class="math display">\[ \theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})\]</span> <span class="math display">\[ \theta_{1} := \theta_{1} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}\]</span></p>
<p>算法的伪代码如下：</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/55414484.jpg" />
</center>
<ul>
<li>由于代价误差函数是二次函数，二次项为负所以图像为“弓（拱）”形，所以是<strong>凸函数</strong>，所以算法总能收敛到全局最小值.</li>
<li>由于训练的过程中，将全部的训练数据喂给我们的模型，所以该过程又称<strong>“批量训练过程”</strong>.</li>
</ul>
</section>
</section>
