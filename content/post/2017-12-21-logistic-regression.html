---
title: 逻辑回归
author: 申恒恒
date: '2017-12-21'
slug: logistic-regression
categories:
  - ml
tags:
  - ml
---



该笔记是来自 Andrew Ng 的 Machine Learning 课程的第三周:<strong>逻辑回归</strong>的课堂记录，逻辑回归是机器学习分类算法的地一个算法，涵盖信息论的相关内容.主要讲解了以下几个内容:

<section class="level1">
<h1>分类算法的应用</h1>
<p>分类的场景在现实社会中无处不在，比如</p>
<ul>
<li>Email:垃圾邮件分类</li>
<li>金融交易是否存在欺诈</li>
<li>肿瘤是恶性/良性</li>
</ul>
<p>分类问题和回归问题一样，只是现在要预测的值只占一小部分离散值。现在，将重点讨论<strong>二分类</strong>问题，其中<span class="math inline">\(Y\)</span>只能接受两个值，<strong>0和1</strong>。（也可以推广到多个类），例如，如果我们试图为电子邮件构建一个垃圾邮件分类器，那么<span class="math inline">\(X^{(i)}\)</span>可能是一封电子邮件的一些特征，如果它是一封垃圾邮件，<span class="math inline">\(y\)</span>是1，否则为0。因此，<span class="math inline">\(Y \in \{0,1\}\)</span>。0也被称为否定类，1是正类，它们有时也用符号<strong>“-”</strong>和<strong>“+”</strong>表示。</p>
</section>
<section class="level1">
<h1>二分类问题</h1>
<p>医疗诊断是机器学习在医学方面其中的一个应用，本小节主要借助乳腺癌诊断这个例子引出.给出一组数据，其中数据特征为肿瘤的大小，标记<span class="math inline">\(y\)</span>是0或者1， 其中0表示恶性，1表示良性，将这些样本数据做图如下：</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/24665725.jpg" />
</center>
<p>我们可以尝试使用线性回归加上一个合适的来实现分类问题，如图所示，<span class="math inline">\(h_{\theta}(x) = \theta^{T}x\)</span>，则基于阈值0.5的分类器算法如下： <span class="math display">\[\begin{align*}
&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \\
&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 
\end{align*}\]</span></p>
<p>但是这将存在一个很大问题，该算法并不具有鲁棒性，即假设我们有观测到了一个非常大的尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中，这样将会我们之前的假说<span class="math inline">\(h\)</span>发生很大变化，如图中最右侧的那个点加入到训练集中，将对<span class="math inline">\(h(x)\)</span>影响很大，这时将0.5作为阈值来预测肿瘤是否为恶性就不再合适了。虽然很容易构造出模型来，但用原来的线性回归模型解决分类问题的这种方法表现很差。直观地说，也没有任何道理，通过图很容易看出，存在<span class="math inline">\(h_{\theta}(x)&gt;1\)</span>或者<span class="math inline">\(h_{\theta}(x)&lt;0\)</span>，但，<span class="math inline">\(Y \in \{0,1\}\)</span>。为了解决这个问题，我们改变<span class="math inline">\(h_{\theta}(x)\)</span>的形式，使得输出变量<span class="math inline">\(0 \leq h_{\theta}(x) \leq 1\)</span>.我们将使用<span class="math inline">\(Sigmoid\)</span>来表示假说函数，也叫为逻辑函数，又因为逻辑函数的输入为<span class="math inline">\(\theta^{T}x\)</span>，所以我们的模型又叫做<strong>逻辑回归模型</strong>.</p>
<p>逻辑回归模型为： <span class="math display">\[\begin{align*}
&amp; h_\theta (x) = g ( \theta^T x )\\
&amp; z = \theta^T x \\
&amp;g(z) = \dfrac{1}{1 + e^{-z}}
\end{align*}\]</span></p>
<p><span class="math inline">\(g(z)\)</span>函数图像如图所示.</p>
<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/53893816.jpg" />
</center>

<p><strong>另外</strong> ，对于给定的数据X，根据选择的参数，<span class="math inline">\(h_{\theta}(x)\)</span>表示<span class="math inline">\(y=1\)</span>发生的可能性。即<span class="math inline">\(h_{\theta}(x) = {P}(y=1 | x,\theta)\)</span>,例如，<span class="math inline">\(h_{\theta}(x)= 0.7\)</span>告诉我们<span class="math inline">\(y=1\)</span>的概率为<span class="math inline">\(70\%\)</span>，那么根据概率基本知识，<span class="math inline">\(y=0\)</span>的概率为<span class="math inline">\(30\%\)</span>.</p>
<p>基本的概率公式： <span class="math display">\[\begin{align*}
&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \\
&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1
\end{align*}\]</span></p>
</section>
<section class="level1">
<h1>边界函数</h1>
<section class="level2">
<h2>线性决策边界</h2>
<p>根据图和上面的公式，有以下结论. <span class="math display">\[\begin{align*}
&amp;z=0, e^{0}=1 \Rightarrow g(z)=1/2 \\
&amp; z \to \infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \\
&amp;z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0 \\
&amp; \theta^{T}x \geq 0 \rightarrow h_\theta(x) \geq 0.5 \rightarrow y = 1 \\
&amp; \theta^{T}x &lt; 0 \rightarrow h_\theta(x) &lt; 0.5 \rightarrow y = 0
\end{align*}\]</span></p>
<p>假设有一个模型，模型表示如下</p>
<span class="math display">\[\begin{align*}
&amp; \theta = 
\begin{bmatrix}
-3 \\
1 \\
1 \end{bmatrix}\\
&amp; y = 1 \; if \; -3 + 1 x_1 + 1 x_2 \geq 0\\
&amp; y = 1 \; if \; x_1 + x_2 \geq 3
\end{align*}\]</span>
画出边界，如下图所示.

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/40568449.jpg" />
</center>
</section>
<section class="level2">
<h2>非线性决策边界</h2>
往往在实际的数据中，数据往往是线性不可分的！所以在此基础之上，引申出非线性决策边界的概念。如图所示，这个边界是一个圆<span class="math inline">\(z = \theta_0 + \theta_1 x_1^2 +\theta_2 x_2^2\)</span>或者一个其他的形状将<span class="math inline">\(y = 0\)</span>的区域和<span class="math inline">\(y = 1\)</span>的区域分开.

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/68833588.jpg" />
</center>
<p>图，模型描述为： <span class="math display">\[\begin{align*}
h_{\theta}(x) &amp;= g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{1}^2 + \theta_{4}x_{2}^2)\\
h_{\theta}(x) &amp;= g(-1 + 0 x_{1} + 0 x_{2} + 1 x_{1}^2 + 1 x_{2}^2)\\
y &amp;= 1 \; if  \; 1 x_{1}^2 + 1 x_{2}^2 \geq 1
\end{align*}\]</span></p>
<p>我们也可以使用比较复杂的模型来适应非常复杂形状的边界.<span class="math inline">\(h_{\theta}(x) = g(\theta_0 + \theta_1 x_1 +\theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2 + \theta_5 x_1x_2 +...)\)</span></p>
</section>
</section>
<section class="level1">
<h1>代价函数</h1>
<p>在定义逻辑回归模型的代价函数中，我们不能使用线性回归模型定义的成本函数，因为<span class="math inline">\(logistic\)</span>函数会导致<span class="math inline">\(y\)</span>出现震荡，从而出现许多局部的最优解，此时定义的代价函数不是一个凸函数。</p>
<p>逻辑回归模型的代价函数为 <span class="math display">\[\begin{align*}
 J(\theta) &amp;= \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)})
\end{align*}\]</span> <span class="math display">\[
\left\{
 \begin{aligned}
 \mathrm{Cost}(h_\theta(x),y) &amp;= -\log(h_\theta(x)) \; &amp; \text{if y = 1} \\
 \mathrm{Cost}(h_\theta(x),y) &amp;= -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}
  \end{aligned}
\right.\]</span></p>
<p>当<span class="math inline">\(y=1\)</span>和<span class="math inline">\(y=0\)</span>时，分别画出 <span class="math inline">\(J(\theta) = \mathrm{Cost}(h_{\theta}(x), y)\)</span>的图像，如图</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/7436806.jpg" />
</center>
<p><span class="math inline">\(\mathrm{Cost}(h_{\theta}(x),y)\)</span>的特点为： <span class="math display">\[\begin{align*}
&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \\
 &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \\
&amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 
\end{align*}\]</span></p>
<p><strong>公式反映了一个事实：</strong> 如果<span class="math inline">\(h_{\theta} = 0\)</span>，即<span class="math inline">\(\mathrm{P}(y=1 | \theta,x)=0\)</span>，但是<span class="math inline">\(\mathrm{y}=1\)</span>，则我们将会以一个很大的代价（<span class="math inline">\(\infty\)</span>）惩罚学习算法！</p>
<p><strong>上面的话简单点:</strong> 你预测对了，没事！（<span class="math inline">\(\mathrm{OK} \rightarrow \mathrm{Cost} = 0\)</span>），但是你如果预测错了，抱歉，狠狠教训一顿！（<span class="math inline">\(\mathrm{Sorry}, \mathrm{Cost} \rightarrow \infty\)</span>），其实这是<strong>信息论中熵的应用，信息增益</strong>！</p>
<p>这样写过之后，就保证了代价函数是凸函数！</p>
</section>
<section class="level1">
<h1>简化代价函数形式和梯度下降算法</h1>
<p>我们可以将 <span class="math display">\[ \mathrm{Cost}(h_\theta(x),y) = 
\left\{
 \begin{aligned}
 &amp; -\log(h_\theta(x)) \; &amp; \text{if y = 1} \\
 &amp; -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}
  \end{aligned}
\right.\]</span></p>
<p>简化为 <span class="math display">\[\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))\]</span></p>
<p>代入代价函数得到： <span class="math display">\[\begin{align*}
J(\theta) &amp;= - \frac{1}{m} \displaystyle \sum_{i=1}^m \mathrm{Cost}(h_{\theta}(x^{(i)}) - y^{(i)}) \\
          &amp;= - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]
\end{align*}\]</span></p>
<p>向量化之后：</p>
<span class="math display">\[\begin{align*} 
&amp; h = g(X\theta)\\
&amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) 
\end{align*}\]</span>
<p>在得到这样的一个代价函数之后，就可以使用梯度下降算法求出使得代价函数最小的参数<span class="math inline">\(\theta\)</span>.</p>
<p>梯度下降算法框架如下： <span class="math display">\[\begin{align*}
&amp; Repeat \; \lbrace \\
&amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \\
 &amp; \rbrace
\end{align*}\]</span></p>
<p>将<span class="math inline">\(J(\theta)\)</span>代入得</p>
<span class="math display">\[\begin{align*} &amp; Repeat \; \lbrace \\
&amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\
&amp; \rbrace
 \end{align*}\]</span>
<p>向量化：<span class="math display">\[\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})\]</span></p>
<p><strong>注意：</strong> 表面上看起来和线性回归的梯度下降一样，但是这里的<span class="math inline">\(h_{\theta}(x)=g(\theta^{T}x)\)</span>与线性回归不同，所以实际上是不一样的。另外在运行梯度下降算法之前，进行特征缩放是非常必要的。</p>
</section>
<section class="level1">
<h1>高级优化</h1>
<p>“共轭梯度”、“BFGS”-局部优化法、和“L-BFGS”-有限内存局部优化，使用更成熟，更快的方法来优化<span class="math inline">\(\theta\)</span>，它们可以用来代替梯度下降。不建议自己编写这些复杂的算法，而是使用库，因为它们已经经过测试并高度优化。Octave提供它们！ <span class="math display">\[\begin{align*}
&amp;\mathrm{Given} \; \theta\\
&amp; \rightarrow J(\theta) \\
&amp; \rightarrow \frac{\partial}{\partial_{\theta_{j}}}J(\theta) \;  \mathrm{for} \;{j} = 0,1,2,3,...
\end{align*}\]</span></p>

<p>代码实现：</p>

<center>
<img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/86473859.jpg" />
</center>
</section>
