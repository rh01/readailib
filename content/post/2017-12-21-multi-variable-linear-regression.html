---
title: 多变量线性回归
author: Heng-Heng Shen
github: rh01
date: '2017-12-21'
slug: multi-variable-linear-regression
categories:
  - ml
tags:
  - ml
---



<p>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第二周:<strong>多变量线性回归</strong>的课堂记录，其实就是将单变量线性回归推广到多变量中去，主要讲解了以下几个内容:</p>
<ul>
<li>模型的表示</li>
<li>梯度下降算法</li>
<li>运算中的实用技巧
<ul>
<li>特征缩放</li>
<li>学习率<span class="math inline">\(\alpha\)</span></li>
</ul></li>
</ul>
<section class="level1">
<h1>模型表达</h1>
在单变量线性回归的房屋预测模型例子中，我们忽略了一个显示，往往购房者不仅仅考虑一个因素，因此多变量（特征）的引入是必需的，比如房屋的价格往往和卧室的数量（numbers of bedrooms），有多少个楼层(numbers of floors)，房屋的年龄(age of home)等等，我们选取这些特征来预测价格，实例数据如图

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/12854191.jpg" /></p>
<p>为了后面描述的方便，特别对符号作出声明：</p>
<ul>
<li><span class="math inline">\(n\)</span> 特征的数目</li>
<li><span class="math inline">\(m\)</span> 训练集中实例的数量</li>
<li><span class="math inline">\(x&#39;s\)</span> 输入变量或者特征</li>
<li><span class="math inline">\(y&#39;s\)</span> 输出变量或目标变量</li>
<li><span class="math inline">\(x^{(i)}\)</span> 第<span class="math inline">\(i\)</span>个训练样本的输入特征</li>
<li><span class="math inline">\(x^{(i)}_{j}\)</span> 第<span class="math inline">\(i\)</span>个训练样本的第<span class="math inline">\(j\)</span>个特征</li>
<li><span class="math inline">\(x_{j}\)</span> 样本数据的第<span class="math inline">\(j\)</span>个特征</li>
</ul>
<p>如图中，第一个训练样本为<span class="math inline">\(x^{(1)}=\left[\begin{matrix} 2104 &amp;5 &amp;1&amp; 45 &amp;460\end{matrix}\right]\)</span> 则第一个训练样本的第一个特征为<span class="math inline">\(x^{(1)}_{1} = 21040\)</span>样本数据的第一个特征 <span class="math inline">\(x_{1}= \left[ \begin{matrix} 2104 \\ 1416 \\ 1534\\ 852 \end{matrix} \right]\)</span></p>
<p>同样地，支持多变量的假说函数表示为<span class="math display">\[h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1}  + \theta_{2}x_{2} + \theta_{3}x_{3} + ...\]</span></p>
<p>为了更方便的表示，令 define <span class="math inline">\(x_{0}\)</span> = 1 <span class="math display">\[ X = \left[\begin{matrix}
x_{0}\\
x_{1}\\
x_{2}\\
x_{3}\\
... \\
x_{n}
\end{matrix}
\right] \in \mathbb{R}^{n+1}, \theta = \left[\begin{matrix}
\theta_{0} \\
\theta_{1} \\
\theta_{2} \\
\theta_{3} \\
...\\
\theta_{n}
\end{matrix}\right]\]</span></p>
<p>所以假说函数<span class="math inline">\(h_{\theta}\)</span>可以表示为<span class="math display">\[h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1}  + \theta_{2}x_{2} + \theta_{3}x_{3} + ...
= \theta^{T}X\]</span></p>
</section>
<section class="level1">
<h1>梯度下降算法</h1>
<p>本节主要讲解如何设定该假设的参数，特别地，讲解如何使用梯度下降算法处理多元线性回归模型。现在我们已经知道：</p>
<ul>
<li>假说函数：<span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + ... = \theta^{T}X\)</span></li>
<li>参数： <span class="math inline">\(\theta \in \mathbb{R}^{n+1}\)</span></li>
<li>代价误差函数：<span class="math inline">\(J(\mathbb{\theta}) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{(i)})^{2}\)</span></li>
<li>Goals：<span class="math inline">\(minimize_{\mathbb{\theta}}J(\mathbb{\theta})\)</span></li>
</ul>
<p>梯度下降算法伪代码： <img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/92129221.jpg" /></p>

</section>
<section class="level1">
<h1>运算中的适用技巧</h1>
<section class="level2">
<h2>特征缩放<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h2>
<p>特征缩放（feature scaling）可以把特征缩放到一个相近的范围内，是一种能使得梯度下降算法快速收敛的一个技巧。</p>
<p>例子：图中，<span class="math inline">\(\theta_{2}\)</span>表示房屋的大小（<span class="math inline">\(0～2000\)</span> <span class="math inline">\(feets\)</span>）， <span class="math inline">\(\theta_{2}\)</span>表示卧室的数量(<span class="math inline">\(0~5\)</span>)，从图(a)可以看出等高线很扁，如果不进行特征的缩放，梯度下降算法需要经过很多次迭代才能到达最小值点。经过缩放后，收敛到最小值速度比不缩放的快。</p>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/78138706.jpg" /></p>
<p>为了实现特征缩放将会使用到一种叫做<strong>均值归一化</strong>的技术，使得<span class="math inline">\(\theta_{1}\)</span>,<span class="math inline">\(\theta_{2}\)</span>的范围在[-1,1]之间。主要的转换公式为： <span class="math display">\[X_{i} = \frac{X_{i} - \mu_{n}}{\mathbb{S}_{n}}\]</span></p>
<p>其中<span class="math inline">\(\mu_{n}\)</span>表示<span class="math inline">\(X\)</span>的均值，<span class="math inline">\(\mathbb{S}_{n}\)</span>表示<span class="math inline">\(X\)</span>的范围，即<span class="math inline">\(max - min\)</span>.</p>
<p>比如<span class="math inline">\(\theta_{1}\)</span>归一化的结果为<span class="math inline">\(\theta_{1} = \frac{\theta_{1} - 1000}{2000}\)</span></p>
</section>
<section id="alpha" class="level2">
<h2>学习率<span class="math inline">\(\alpha\)</span></h2>
<p>复习：梯度下降算法的迭代公式如下.</p>
<p><span class="math display">\[\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial_{\theta_{j}}}J(\theta)\]</span></p>
<p>通过上面的公式，我们可以绘制出迭代次数与代价函数<span class="math inline">\(J(\theta)\)</span>的关系曲线.通过曲线观测算发在何时起开始收敛。也有一些自动测试是否收敛的方法，例如将代价函数的变化与某个阈值（例如0.01）进行比较，通常图能直观的表示出<span class="math inline">\(J(\theta)\)</span>的变化情况。</p>

<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/99087502.jpg" /></p>
<p>那么上面的例子与学习率有什么关系？关系就是如果学习率<span class="math inline">\(\alpha\)</span>选择不合适将会导致梯度算法不能工作，即不能到达最小值或者将会使得梯度下降算法运行效率低迟迟不能收敛！</p>
<p><strong>总结：</strong></p>
<ul>
<li>如果<span class="math inline">\(\alpha\)</span>过小，将会导致算法的收敛速度慢;</li>
<li>如果<span class="math inline">\(\alpha\)</span>过大，在梯度下降过程中，很容易掠过局部最小值，导致无法收敛甚至发散。如图.</li>
</ul>
<p><strong>技巧：</strong> 可以尝试下面的学习率：<span class="math display">\[\alpha = 0.001,0.003,0.01,0.03,0.1,0.3,1\]</span></p>
</section>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>特征归一化<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>
